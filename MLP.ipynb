{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae20a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5579d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"csv/MediaPipe/train_nose_184_cropped_8emotion.csv\")\n",
    "test = pd.read_csv(\"csv/MediaPipe/test_nose_184_cropped_8emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673c3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(\"y\", axis=1)\n",
    "y_train = train[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b0240a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V174</th>\n",
       "      <th>V175</th>\n",
       "      <th>V176</th>\n",
       "      <th>V177</th>\n",
       "      <th>V178</th>\n",
       "      <th>V179</th>\n",
       "      <th>V180</th>\n",
       "      <th>V181</th>\n",
       "      <th>V182</th>\n",
       "      <th>V183</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858615</td>\n",
       "      <td>0.825387</td>\n",
       "      <td>0.774809</td>\n",
       "      <td>0.722167</td>\n",
       "      <td>0.683341</td>\n",
       "      <td>0.622038</td>\n",
       "      <td>0.637829</td>\n",
       "      <td>0.836358</td>\n",
       "      <td>0.852549</td>\n",
       "      <td>0.652842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.865015</td>\n",
       "      <td>-1.172562</td>\n",
       "      <td>-1.154096</td>\n",
       "      <td>-0.977151</td>\n",
       "      <td>-0.055176</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>-0.376445</td>\n",
       "      <td>-0.350426</td>\n",
       "      <td>-0.564401</td>\n",
       "      <td>-0.199428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.096722</td>\n",
       "      <td>0.149883</td>\n",
       "      <td>0.210612</td>\n",
       "      <td>0.319156</td>\n",
       "      <td>0.418227</td>\n",
       "      <td>0.497839</td>\n",
       "      <td>0.541343</td>\n",
       "      <td>0.100233</td>\n",
       "      <td>0.088916</td>\n",
       "      <td>0.482932</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.534856</td>\n",
       "      <td>-1.471007</td>\n",
       "      <td>-1.185580</td>\n",
       "      <td>-0.782679</td>\n",
       "      <td>-0.815739</td>\n",
       "      <td>-0.665111</td>\n",
       "      <td>-1.123142</td>\n",
       "      <td>-1.295908</td>\n",
       "      <td>-1.337882</td>\n",
       "      <td>-1.239920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.473112</td>\n",
       "      <td>0.523687</td>\n",
       "      <td>0.552028</td>\n",
       "      <td>0.565787</td>\n",
       "      <td>0.565893</td>\n",
       "      <td>0.551039</td>\n",
       "      <td>0.555815</td>\n",
       "      <td>0.392295</td>\n",
       "      <td>0.417814</td>\n",
       "      <td>0.560580</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446299</td>\n",
       "      <td>-0.564322</td>\n",
       "      <td>-0.477542</td>\n",
       "      <td>-0.251394</td>\n",
       "      <td>-0.221608</td>\n",
       "      <td>-0.713836</td>\n",
       "      <td>-0.674373</td>\n",
       "      <td>-0.389941</td>\n",
       "      <td>-0.550756</td>\n",
       "      <td>-0.125124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309079</td>\n",
       "      <td>0.375406</td>\n",
       "      <td>0.384272</td>\n",
       "      <td>0.380315</td>\n",
       "      <td>0.366400</td>\n",
       "      <td>0.362447</td>\n",
       "      <td>0.347218</td>\n",
       "      <td>0.189191</td>\n",
       "      <td>0.221457</td>\n",
       "      <td>0.355136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640944</td>\n",
       "      <td>-0.717027</td>\n",
       "      <td>-0.590346</td>\n",
       "      <td>-0.400920</td>\n",
       "      <td>-0.934616</td>\n",
       "      <td>-0.843371</td>\n",
       "      <td>-0.805098</td>\n",
       "      <td>-0.905101</td>\n",
       "      <td>-0.698863</td>\n",
       "      <td>-0.532940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.101826</td>\n",
       "      <td>-1.009243</td>\n",
       "      <td>-0.951218</td>\n",
       "      <td>-0.919509</td>\n",
       "      <td>-0.924826</td>\n",
       "      <td>-1.172693</td>\n",
       "      <td>-0.959095</td>\n",
       "      <td>-1.283163</td>\n",
       "      <td>-1.222229</td>\n",
       "      <td>-0.936492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627450</td>\n",
       "      <td>1.586391</td>\n",
       "      <td>1.996306</td>\n",
       "      <td>2.072256</td>\n",
       "      <td>0.253576</td>\n",
       "      <td>0.413474</td>\n",
       "      <td>0.526993</td>\n",
       "      <td>0.441075</td>\n",
       "      <td>0.518837</td>\n",
       "      <td>0.031181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V0        V1        V2        V3        V4        V5        V6   \n",
       "0  0.858615  0.825387  0.774809  0.722167  0.683341  0.622038  0.637829  \\\n",
       "1  0.096722  0.149883  0.210612  0.319156  0.418227  0.497839  0.541343   \n",
       "2  0.473112  0.523687  0.552028  0.565787  0.565893  0.551039  0.555815   \n",
       "3  0.309079  0.375406  0.384272  0.380315  0.366400  0.362447  0.347218   \n",
       "4 -1.101826 -1.009243 -0.951218 -0.919509 -0.924826 -1.172693 -0.959095   \n",
       "\n",
       "         V7        V8        V9  ...      V174      V175      V176      V177   \n",
       "0  0.836358  0.852549  0.652842  ... -0.865015 -1.172562 -1.154096 -0.977151  \\\n",
       "1  0.100233  0.088916  0.482932  ... -1.534856 -1.471007 -1.185580 -0.782679   \n",
       "2  0.392295  0.417814  0.560580  ... -0.446299 -0.564322 -0.477542 -0.251394   \n",
       "3  0.189191  0.221457  0.355136  ... -0.640944 -0.717027 -0.590346 -0.400920   \n",
       "4 -1.283163 -1.222229 -0.936492  ...  0.627450  1.586391  1.996306  2.072256   \n",
       "\n",
       "       V178      V179      V180      V181      V182      V183  \n",
       "0 -0.055176  0.001249 -0.376445 -0.350426 -0.564401 -0.199428  \n",
       "1 -0.815739 -0.665111 -1.123142 -1.295908 -1.337882 -1.239920  \n",
       "2 -0.221608 -0.713836 -0.674373 -0.389941 -0.550756 -0.125124  \n",
       "3 -0.934616 -0.843371 -0.805098 -0.905101 -0.698863 -0.532940  \n",
       "4  0.253576  0.413474  0.526993  0.441075  0.518837  0.031181  \n",
       "\n",
       "[5 rows x 184 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aff41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(\"y\", axis=1)\n",
    "y_test = test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654459b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V174</th>\n",
       "      <th>V175</th>\n",
       "      <th>V176</th>\n",
       "      <th>V177</th>\n",
       "      <th>V178</th>\n",
       "      <th>V179</th>\n",
       "      <th>V180</th>\n",
       "      <th>V181</th>\n",
       "      <th>V182</th>\n",
       "      <th>V183</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159450</td>\n",
       "      <td>0.092518</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>0.154854</td>\n",
       "      <td>0.238034</td>\n",
       "      <td>0.333146</td>\n",
       "      <td>0.352455</td>\n",
       "      <td>0.293689</td>\n",
       "      <td>0.254175</td>\n",
       "      <td>0.302080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019967</td>\n",
       "      <td>-0.338051</td>\n",
       "      <td>-0.493751</td>\n",
       "      <td>-0.578819</td>\n",
       "      <td>0.831001</td>\n",
       "      <td>0.745222</td>\n",
       "      <td>0.570051</td>\n",
       "      <td>0.600601</td>\n",
       "      <td>0.362707</td>\n",
       "      <td>0.325782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.400809</td>\n",
       "      <td>-0.612326</td>\n",
       "      <td>-0.846924</td>\n",
       "      <td>-1.123699</td>\n",
       "      <td>-1.326612</td>\n",
       "      <td>-1.518536</td>\n",
       "      <td>-1.522389</td>\n",
       "      <td>-0.179601</td>\n",
       "      <td>-0.260954</td>\n",
       "      <td>-1.450057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305114</td>\n",
       "      <td>1.107279</td>\n",
       "      <td>1.316345</td>\n",
       "      <td>1.345083</td>\n",
       "      <td>-0.292861</td>\n",
       "      <td>-0.132299</td>\n",
       "      <td>0.132239</td>\n",
       "      <td>-0.024831</td>\n",
       "      <td>0.192830</td>\n",
       "      <td>-0.158113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.502503</td>\n",
       "      <td>-0.492592</td>\n",
       "      <td>-0.485455</td>\n",
       "      <td>-0.490099</td>\n",
       "      <td>-0.490608</td>\n",
       "      <td>-0.497583</td>\n",
       "      <td>-0.482947</td>\n",
       "      <td>-0.547429</td>\n",
       "      <td>-0.516347</td>\n",
       "      <td>-0.488743</td>\n",
       "      <td>...</td>\n",
       "      <td>1.359535</td>\n",
       "      <td>0.452661</td>\n",
       "      <td>-0.024015</td>\n",
       "      <td>-0.343805</td>\n",
       "      <td>2.930198</td>\n",
       "      <td>2.787786</td>\n",
       "      <td>2.453542</td>\n",
       "      <td>2.520344</td>\n",
       "      <td>2.099021</td>\n",
       "      <td>2.624696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.390620</td>\n",
       "      <td>0.276295</td>\n",
       "      <td>0.191752</td>\n",
       "      <td>0.154475</td>\n",
       "      <td>0.173530</td>\n",
       "      <td>0.242542</td>\n",
       "      <td>0.219779</td>\n",
       "      <td>0.548233</td>\n",
       "      <td>0.493273</td>\n",
       "      <td>0.196168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568672</td>\n",
       "      <td>0.617128</td>\n",
       "      <td>0.655726</td>\n",
       "      <td>0.600655</td>\n",
       "      <td>1.547585</td>\n",
       "      <td>1.740032</td>\n",
       "      <td>1.269748</td>\n",
       "      <td>1.157858</td>\n",
       "      <td>0.968095</td>\n",
       "      <td>0.232494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.623326</td>\n",
       "      <td>0.610391</td>\n",
       "      <td>0.623909</td>\n",
       "      <td>0.708758</td>\n",
       "      <td>0.808036</td>\n",
       "      <td>1.023063</td>\n",
       "      <td>0.956635</td>\n",
       "      <td>0.716895</td>\n",
       "      <td>0.685492</td>\n",
       "      <td>0.884659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.604208</td>\n",
       "      <td>-0.331344</td>\n",
       "      <td>-0.055060</td>\n",
       "      <td>0.157113</td>\n",
       "      <td>-0.340324</td>\n",
       "      <td>-0.342361</td>\n",
       "      <td>-0.545417</td>\n",
       "      <td>-0.678132</td>\n",
       "      <td>-0.572869</td>\n",
       "      <td>-0.510297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V0        V1        V2        V3        V4        V5        V6   \n",
       "0  0.159450  0.092518  0.086819  0.154854  0.238034  0.333146  0.352455  \\\n",
       "1 -0.400809 -0.612326 -0.846924 -1.123699 -1.326612 -1.518536 -1.522389   \n",
       "2 -0.502503 -0.492592 -0.485455 -0.490099 -0.490608 -0.497583 -0.482947   \n",
       "3  0.390620  0.276295  0.191752  0.154475  0.173530  0.242542  0.219779   \n",
       "4  0.623326  0.610391  0.623909  0.708758  0.808036  1.023063  0.956635   \n",
       "\n",
       "         V7        V8        V9  ...      V174      V175      V176      V177   \n",
       "0  0.293689  0.254175  0.302080  ...  0.019967 -0.338051 -0.493751 -0.578819  \\\n",
       "1 -0.179601 -0.260954 -1.450057  ...  0.305114  1.107279  1.316345  1.345083   \n",
       "2 -0.547429 -0.516347 -0.488743  ...  1.359535  0.452661 -0.024015 -0.343805   \n",
       "3  0.548233  0.493273  0.196168  ...  0.568672  0.617128  0.655726  0.600655   \n",
       "4  0.716895  0.685492  0.884659  ... -0.604208 -0.331344 -0.055060  0.157113   \n",
       "\n",
       "       V178      V179      V180      V181      V182      V183  \n",
       "0  0.831001  0.745222  0.570051  0.600601  0.362707  0.325782  \n",
       "1 -0.292861 -0.132299  0.132239 -0.024831  0.192830 -0.158113  \n",
       "2  2.930198  2.787786  2.453542  2.520344  2.099021  2.624696  \n",
       "3  1.547585  1.740032  1.269748  1.157858  0.968095  0.232494  \n",
       "4 -0.340324 -0.342361 -0.545417 -0.678132 -0.572869 -0.510297  \n",
       "\n",
       "[5 rows x 184 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42d79466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = y_train.nunique()\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32560928",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab10099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "508e2592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dadf42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler().fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d6d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = np.sqrt(scaler.var_)\n",
    "# np.save(\"std5.npy\", std)\n",
    "# np.save(\"mean5.npy\", scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22764ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2, l1\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import RMSprop, SGD, Adagrad, Adam\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "686f8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_decay(epoch):\n",
    "#     initial_lr = 0.01\n",
    "#     lr = initial_lr * 1.0 / (1.0 + 0.001 * epoch)\n",
    "#     return lr\n",
    "\n",
    "# def lr_decay(epoch):\n",
    "#     lr = 0.0003\n",
    "#     if epoch > 1:\n",
    "#         lr = 0.0001\n",
    "#     if epoch > 10:\n",
    "#         lr = 0.00005\n",
    "#     if epoch > 40:\n",
    "#         lr = 0.00003\n",
    "#     if epoch > 50:\n",
    "#         lr = 0.00001\n",
    "#     if epoch > 90:\n",
    "#         lr = 0.000005\n",
    "#     if epoch > 120:\n",
    "#         lr = 0.000003\n",
    "#     if epoch > 130:\n",
    "#         lr = 0.000001\n",
    "#     return lr\n",
    "\n",
    "def lr_decay(epoch):\n",
    "    lr = 0.0003\n",
    "    if epoch > 1:\n",
    "        lr = 0.0001\n",
    "    if epoch > 26:\n",
    "        lr = 0.00005\n",
    "    if epoch > 40:\n",
    "        lr = 0.00003\n",
    "    if epoch > 50:\n",
    "        lr = 0.00001\n",
    "    if epoch > 90:\n",
    "        lr = 0.000005\n",
    "    if epoch > 120:\n",
    "        lr = 0.000003\n",
    "    if epoch > 130:\n",
    "        lr = 0.000001\n",
    "    if epoch > 150:\n",
    "        lr = 0.0000005\n",
    "    if epoch > 160:\n",
    "        lr = 0.0000003\n",
    "    if epoch > 170:\n",
    "        lr = 0.0000001\n",
    "    if epoch > 180:\n",
    "        lr = 0.00000005\n",
    "    if epoch > 190:\n",
    "        lr = 0.00000001\n",
    "    return lr * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "660bae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateScheduler(lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1809cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(32, input_dim=24, activation='elu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dense(64, activation='elu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dense(128, activation='elu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='elu'))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation='elu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, activation='elu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "#model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a2baec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cf47ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47a8e5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 64)                11840     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 8)                 2056      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,032\n",
      "Trainable params: 56,136\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6ab5895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "657/657 [==============================] - 8s 11ms/step - loss: 4.3859 - accuracy: 0.3018 - val_loss: 3.9718 - val_accuracy: 0.3979 - lr: 0.0030\n",
      "Epoch 2/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 4.0038 - accuracy: 0.3586 - val_loss: 3.7648 - val_accuracy: 0.4142 - lr: 0.0030\n",
      "Epoch 3/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.8355 - accuracy: 0.3756 - val_loss: 3.6964 - val_accuracy: 0.4119 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.7671 - accuracy: 0.3763 - val_loss: 3.6373 - val_accuracy: 0.4171 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.7020 - accuracy: 0.3825 - val_loss: 3.5801 - val_accuracy: 0.4154 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.6418 - accuracy: 0.3842 - val_loss: 3.5200 - val_accuracy: 0.4208 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "657/657 [==============================] - 8s 11ms/step - loss: 3.5783 - accuracy: 0.3873 - val_loss: 3.4606 - val_accuracy: 0.4228 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.5161 - accuracy: 0.3921 - val_loss: 3.4108 - val_accuracy: 0.4185 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.4622 - accuracy: 0.3913 - val_loss: 3.3610 - val_accuracy: 0.4216 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.4068 - accuracy: 0.3976 - val_loss: 3.3069 - val_accuracy: 0.4282 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.3559 - accuracy: 0.3975 - val_loss: 3.2580 - val_accuracy: 0.4299 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 3.3067 - accuracy: 0.4003 - val_loss: 3.2116 - val_accuracy: 0.4293 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "657/657 [==============================] - 8s 13ms/step - loss: 3.2607 - accuracy: 0.3969 - val_loss: 3.1662 - val_accuracy: 0.4299 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 3.2105 - accuracy: 0.4021 - val_loss: 3.1179 - val_accuracy: 0.4234 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "657/657 [==============================] - 8s 11ms/step - loss: 3.1651 - accuracy: 0.4020 - val_loss: 3.0756 - val_accuracy: 0.4299 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.1187 - accuracy: 0.4059 - val_loss: 3.0356 - val_accuracy: 0.4279 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.0719 - accuracy: 0.4098 - val_loss: 2.9919 - val_accuracy: 0.4305 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 3.0324 - accuracy: 0.4131 - val_loss: 2.9520 - val_accuracy: 0.4285 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.9909 - accuracy: 0.4107 - val_loss: 2.9093 - val_accuracy: 0.4262 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 2.9475 - accuracy: 0.4108 - val_loss: 2.8734 - val_accuracy: 0.4288 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "657/657 [==============================] - 8s 13ms/step - loss: 2.9164 - accuracy: 0.4127 - val_loss: 2.8368 - val_accuracy: 0.4299 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "657/657 [==============================] - 9s 14ms/step - loss: 2.8765 - accuracy: 0.4136 - val_loss: 2.8012 - val_accuracy: 0.4308 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "657/657 [==============================] - 9s 13ms/step - loss: 2.8361 - accuracy: 0.4142 - val_loss: 2.7655 - val_accuracy: 0.4299 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 2.8034 - accuracy: 0.4116 - val_loss: 2.7293 - val_accuracy: 0.4316 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.7708 - accuracy: 0.4125 - val_loss: 2.6974 - val_accuracy: 0.4339 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.7396 - accuracy: 0.4165 - val_loss: 2.6700 - val_accuracy: 0.4336 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.7059 - accuracy: 0.4180 - val_loss: 2.6331 - val_accuracy: 0.4353 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 2.6765 - accuracy: 0.4193 - val_loss: 2.6141 - val_accuracy: 0.4339 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.6681 - accuracy: 0.4169 - val_loss: 2.6019 - val_accuracy: 0.4336 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 2.6496 - accuracy: 0.4188 - val_loss: 2.5843 - val_accuracy: 0.4416 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.6310 - accuracy: 0.4225 - val_loss: 2.5697 - val_accuracy: 0.4365 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.6206 - accuracy: 0.4179 - val_loss: 2.5560 - val_accuracy: 0.4362 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.6004 - accuracy: 0.4198 - val_loss: 2.5386 - val_accuracy: 0.4362 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5803 - accuracy: 0.4247 - val_loss: 2.5263 - val_accuracy: 0.4411 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5726 - accuracy: 0.4208 - val_loss: 2.5134 - val_accuracy: 0.4328 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5630 - accuracy: 0.4157 - val_loss: 2.4980 - val_accuracy: 0.4376 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5398 - accuracy: 0.4245 - val_loss: 2.4852 - val_accuracy: 0.4385 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5305 - accuracy: 0.4234 - val_loss: 2.4737 - val_accuracy: 0.4356 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5157 - accuracy: 0.4263 - val_loss: 2.4592 - val_accuracy: 0.4425 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.5007 - accuracy: 0.4227 - val_loss: 2.4458 - val_accuracy: 0.4382 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4917 - accuracy: 0.4211 - val_loss: 2.4327 - val_accuracy: 0.4339 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4795 - accuracy: 0.4213 - val_loss: 2.4236 - val_accuracy: 0.4371 - lr: 3.0000e-04\n",
      "Epoch 43/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4713 - accuracy: 0.4218 - val_loss: 2.4176 - val_accuracy: 0.4391 - lr: 3.0000e-04\n",
      "Epoch 44/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4634 - accuracy: 0.4236 - val_loss: 2.4086 - val_accuracy: 0.4402 - lr: 3.0000e-04\n",
      "Epoch 45/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4516 - accuracy: 0.4242 - val_loss: 2.4013 - val_accuracy: 0.4393 - lr: 3.0000e-04\n",
      "Epoch 46/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4431 - accuracy: 0.4261 - val_loss: 2.3955 - val_accuracy: 0.4408 - lr: 3.0000e-04\n",
      "Epoch 47/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4328 - accuracy: 0.4294 - val_loss: 2.3855 - val_accuracy: 0.4416 - lr: 3.0000e-04\n",
      "Epoch 48/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4298 - accuracy: 0.4275 - val_loss: 2.3800 - val_accuracy: 0.4405 - lr: 3.0000e-04\n",
      "Epoch 49/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4169 - accuracy: 0.4266 - val_loss: 2.3704 - val_accuracy: 0.4425 - lr: 3.0000e-04\n",
      "Epoch 50/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4181 - accuracy: 0.4283 - val_loss: 2.3647 - val_accuracy: 0.4391 - lr: 3.0000e-04\n",
      "Epoch 51/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4067 - accuracy: 0.4287 - val_loss: 2.3570 - val_accuracy: 0.4396 - lr: 3.0000e-04\n",
      "Epoch 52/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 7s 11ms/step - loss: 2.4023 - accuracy: 0.4229 - val_loss: 2.3542 - val_accuracy: 0.4408 - lr: 1.0000e-04\n",
      "Epoch 53/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3948 - accuracy: 0.4312 - val_loss: 2.3513 - val_accuracy: 0.4430 - lr: 1.0000e-04\n",
      "Epoch 54/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3959 - accuracy: 0.4264 - val_loss: 2.3485 - val_accuracy: 0.4436 - lr: 1.0000e-04\n",
      "Epoch 55/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3950 - accuracy: 0.4287 - val_loss: 2.3467 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 56/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3877 - accuracy: 0.4311 - val_loss: 2.3457 - val_accuracy: 0.4413 - lr: 1.0000e-04\n",
      "Epoch 57/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3871 - accuracy: 0.4277 - val_loss: 2.3422 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 58/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3847 - accuracy: 0.4285 - val_loss: 2.3398 - val_accuracy: 0.4402 - lr: 1.0000e-04\n",
      "Epoch 59/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3914 - accuracy: 0.4250 - val_loss: 2.3370 - val_accuracy: 0.4413 - lr: 1.0000e-04\n",
      "Epoch 60/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3832 - accuracy: 0.4267 - val_loss: 2.3350 - val_accuracy: 0.4425 - lr: 1.0000e-04\n",
      "Epoch 61/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3796 - accuracy: 0.4238 - val_loss: 2.3336 - val_accuracy: 0.4422 - lr: 1.0000e-04\n",
      "Epoch 62/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3765 - accuracy: 0.4307 - val_loss: 2.3305 - val_accuracy: 0.4439 - lr: 1.0000e-04\n",
      "Epoch 63/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3728 - accuracy: 0.4282 - val_loss: 2.3291 - val_accuracy: 0.4399 - lr: 1.0000e-04\n",
      "Epoch 64/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3732 - accuracy: 0.4277 - val_loss: 2.3263 - val_accuracy: 0.4399 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3702 - accuracy: 0.4279 - val_loss: 2.3240 - val_accuracy: 0.4428 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3687 - accuracy: 0.4260 - val_loss: 2.3223 - val_accuracy: 0.4425 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3678 - accuracy: 0.4267 - val_loss: 2.3194 - val_accuracy: 0.4416 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3673 - accuracy: 0.4259 - val_loss: 2.3168 - val_accuracy: 0.4416 - lr: 1.0000e-04\n",
      "Epoch 69/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3596 - accuracy: 0.4289 - val_loss: 2.3142 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 70/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3568 - accuracy: 0.4325 - val_loss: 2.3146 - val_accuracy: 0.4388 - lr: 1.0000e-04\n",
      "Epoch 71/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 2.3569 - accuracy: 0.4288 - val_loss: 2.3100 - val_accuracy: 0.4428 - lr: 1.0000e-04\n",
      "Epoch 72/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3502 - accuracy: 0.4268 - val_loss: 2.3085 - val_accuracy: 0.4413 - lr: 1.0000e-04\n",
      "Epoch 73/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3546 - accuracy: 0.4244 - val_loss: 2.3064 - val_accuracy: 0.4425 - lr: 1.0000e-04\n",
      "Epoch 74/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3520 - accuracy: 0.4267 - val_loss: 2.3041 - val_accuracy: 0.4436 - lr: 1.0000e-04\n",
      "Epoch 75/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3464 - accuracy: 0.4313 - val_loss: 2.3014 - val_accuracy: 0.4422 - lr: 1.0000e-04\n",
      "Epoch 76/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3453 - accuracy: 0.4297 - val_loss: 2.3000 - val_accuracy: 0.4422 - lr: 1.0000e-04\n",
      "Epoch 77/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3429 - accuracy: 0.4238 - val_loss: 2.2983 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 78/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3448 - accuracy: 0.4261 - val_loss: 2.2957 - val_accuracy: 0.4428 - lr: 1.0000e-04\n",
      "Epoch 79/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3453 - accuracy: 0.4290 - val_loss: 2.2930 - val_accuracy: 0.4433 - lr: 1.0000e-04\n",
      "Epoch 80/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3350 - accuracy: 0.4273 - val_loss: 2.2920 - val_accuracy: 0.4416 - lr: 1.0000e-04\n",
      "Epoch 81/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3351 - accuracy: 0.4293 - val_loss: 2.2884 - val_accuracy: 0.4408 - lr: 1.0000e-04\n",
      "Epoch 82/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3296 - accuracy: 0.4286 - val_loss: 2.2863 - val_accuracy: 0.4425 - lr: 1.0000e-04\n",
      "Epoch 83/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3320 - accuracy: 0.4284 - val_loss: 2.2837 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 84/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3259 - accuracy: 0.4316 - val_loss: 2.2821 - val_accuracy: 0.4422 - lr: 1.0000e-04\n",
      "Epoch 85/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3238 - accuracy: 0.4295 - val_loss: 2.2803 - val_accuracy: 0.4430 - lr: 1.0000e-04\n",
      "Epoch 86/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3214 - accuracy: 0.4297 - val_loss: 2.2773 - val_accuracy: 0.4442 - lr: 1.0000e-04\n",
      "Epoch 87/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3195 - accuracy: 0.4293 - val_loss: 2.2763 - val_accuracy: 0.4439 - lr: 1.0000e-04\n",
      "Epoch 88/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3147 - accuracy: 0.4324 - val_loss: 2.2728 - val_accuracy: 0.4419 - lr: 1.0000e-04\n",
      "Epoch 89/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3158 - accuracy: 0.4296 - val_loss: 2.2707 - val_accuracy: 0.4425 - lr: 1.0000e-04\n",
      "Epoch 90/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3138 - accuracy: 0.4309 - val_loss: 2.2684 - val_accuracy: 0.4425 - lr: 1.0000e-04\n",
      "Epoch 91/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3064 - accuracy: 0.4296 - val_loss: 2.2669 - val_accuracy: 0.4405 - lr: 1.0000e-04\n",
      "Epoch 92/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3103 - accuracy: 0.4305 - val_loss: 2.2668 - val_accuracy: 0.4433 - lr: 5.0000e-05\n",
      "Epoch 93/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3123 - accuracy: 0.4281 - val_loss: 2.2648 - val_accuracy: 0.4436 - lr: 5.0000e-05\n",
      "Epoch 94/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3074 - accuracy: 0.4332 - val_loss: 2.2642 - val_accuracy: 0.4425 - lr: 5.0000e-05\n",
      "Epoch 95/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3055 - accuracy: 0.4308 - val_loss: 2.2618 - val_accuracy: 0.4439 - lr: 5.0000e-05\n",
      "Epoch 96/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3088 - accuracy: 0.4283 - val_loss: 2.2613 - val_accuracy: 0.4419 - lr: 5.0000e-05\n",
      "Epoch 97/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2996 - accuracy: 0.4304 - val_loss: 2.2597 - val_accuracy: 0.4422 - lr: 5.0000e-05\n",
      "Epoch 98/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2996 - accuracy: 0.4283 - val_loss: 2.2597 - val_accuracy: 0.4439 - lr: 5.0000e-05\n",
      "Epoch 99/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3064 - accuracy: 0.4287 - val_loss: 2.2590 - val_accuracy: 0.4419 - lr: 5.0000e-05\n",
      "Epoch 100/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3103 - accuracy: 0.4248 - val_loss: 2.2589 - val_accuracy: 0.4459 - lr: 5.0000e-05\n",
      "Epoch 101/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2995 - accuracy: 0.4300 - val_loss: 2.2560 - val_accuracy: 0.4411 - lr: 5.0000e-05\n",
      "Epoch 102/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.3007 - accuracy: 0.4314 - val_loss: 2.2546 - val_accuracy: 0.4405 - lr: 5.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2997 - accuracy: 0.4262 - val_loss: 2.2536 - val_accuracy: 0.4411 - lr: 5.0000e-05\n",
      "Epoch 104/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2990 - accuracy: 0.4282 - val_loss: 2.2531 - val_accuracy: 0.4419 - lr: 5.0000e-05\n",
      "Epoch 105/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2951 - accuracy: 0.4334 - val_loss: 2.2527 - val_accuracy: 0.4413 - lr: 5.0000e-05\n",
      "Epoch 106/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2971 - accuracy: 0.4315 - val_loss: 2.2505 - val_accuracy: 0.4413 - lr: 5.0000e-05\n",
      "Epoch 107/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2937 - accuracy: 0.4268 - val_loss: 2.2503 - val_accuracy: 0.4433 - lr: 5.0000e-05\n",
      "Epoch 108/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2896 - accuracy: 0.4314 - val_loss: 2.2487 - val_accuracy: 0.4416 - lr: 5.0000e-05\n",
      "Epoch 109/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2879 - accuracy: 0.4306 - val_loss: 2.2482 - val_accuracy: 0.4408 - lr: 5.0000e-05\n",
      "Epoch 110/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2920 - accuracy: 0.4297 - val_loss: 2.2462 - val_accuracy: 0.4430 - lr: 5.0000e-05\n",
      "Epoch 111/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2872 - accuracy: 0.4302 - val_loss: 2.2441 - val_accuracy: 0.4436 - lr: 5.0000e-05\n",
      "Epoch 112/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2898 - accuracy: 0.4306 - val_loss: 2.2438 - val_accuracy: 0.4428 - lr: 5.0000e-05\n",
      "Epoch 113/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2879 - accuracy: 0.4346 - val_loss: 2.2439 - val_accuracy: 0.4425 - lr: 5.0000e-05\n",
      "Epoch 114/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2869 - accuracy: 0.4287 - val_loss: 2.2433 - val_accuracy: 0.4445 - lr: 5.0000e-05\n",
      "Epoch 115/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2876 - accuracy: 0.4303 - val_loss: 2.2413 - val_accuracy: 0.4413 - lr: 5.0000e-05\n",
      "Epoch 116/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2828 - accuracy: 0.4337 - val_loss: 2.2409 - val_accuracy: 0.4439 - lr: 5.0000e-05\n",
      "Epoch 117/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2872 - accuracy: 0.4290 - val_loss: 2.2393 - val_accuracy: 0.4453 - lr: 5.0000e-05\n",
      "Epoch 118/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2773 - accuracy: 0.4316 - val_loss: 2.2375 - val_accuracy: 0.4413 - lr: 5.0000e-05\n",
      "Epoch 119/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2804 - accuracy: 0.4288 - val_loss: 2.2370 - val_accuracy: 0.4425 - lr: 5.0000e-05\n",
      "Epoch 120/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2834 - accuracy: 0.4296 - val_loss: 2.2372 - val_accuracy: 0.4436 - lr: 5.0000e-05\n",
      "Epoch 121/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2772 - accuracy: 0.4313 - val_loss: 2.2342 - val_accuracy: 0.4422 - lr: 5.0000e-05\n",
      "Epoch 122/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2768 - accuracy: 0.4335 - val_loss: 2.2348 - val_accuracy: 0.4433 - lr: 3.0000e-05\n",
      "Epoch 123/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2743 - accuracy: 0.4311 - val_loss: 2.2349 - val_accuracy: 0.4448 - lr: 3.0000e-05\n",
      "Epoch 124/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2752 - accuracy: 0.4278 - val_loss: 2.2327 - val_accuracy: 0.4428 - lr: 3.0000e-05\n",
      "Epoch 125/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2735 - accuracy: 0.4287 - val_loss: 2.2330 - val_accuracy: 0.4448 - lr: 3.0000e-05\n",
      "Epoch 126/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2683 - accuracy: 0.4340 - val_loss: 2.2318 - val_accuracy: 0.4442 - lr: 3.0000e-05\n",
      "Epoch 127/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2755 - accuracy: 0.4318 - val_loss: 2.2318 - val_accuracy: 0.4448 - lr: 3.0000e-05\n",
      "Epoch 128/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2733 - accuracy: 0.4297 - val_loss: 2.2314 - val_accuracy: 0.4430 - lr: 3.0000e-05\n",
      "Epoch 129/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2750 - accuracy: 0.4302 - val_loss: 2.2307 - val_accuracy: 0.4442 - lr: 3.0000e-05\n",
      "Epoch 130/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2728 - accuracy: 0.4302 - val_loss: 2.2303 - val_accuracy: 0.4433 - lr: 3.0000e-05\n",
      "Epoch 131/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2717 - accuracy: 0.4306 - val_loss: 2.2294 - val_accuracy: 0.4448 - lr: 3.0000e-05\n",
      "Epoch 132/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2710 - accuracy: 0.4321 - val_loss: 2.2292 - val_accuracy: 0.4436 - lr: 1.0000e-05\n",
      "Epoch 133/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2665 - accuracy: 0.4310 - val_loss: 2.2282 - val_accuracy: 0.4436 - lr: 1.0000e-05\n",
      "Epoch 134/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2751 - accuracy: 0.4317 - val_loss: 2.2286 - val_accuracy: 0.4442 - lr: 1.0000e-05\n",
      "Epoch 135/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2687 - accuracy: 0.4322 - val_loss: 2.2286 - val_accuracy: 0.4436 - lr: 1.0000e-05\n",
      "Epoch 136/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2715 - accuracy: 0.4329 - val_loss: 2.2290 - val_accuracy: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 137/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2706 - accuracy: 0.4326 - val_loss: 2.2276 - val_accuracy: 0.4445 - lr: 1.0000e-05\n",
      "Epoch 138/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2675 - accuracy: 0.4345 - val_loss: 2.2280 - val_accuracy: 0.4465 - lr: 1.0000e-05\n",
      "Epoch 139/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2722 - accuracy: 0.4291 - val_loss: 2.2274 - val_accuracy: 0.4430 - lr: 1.0000e-05\n",
      "Epoch 140/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2659 - accuracy: 0.4334 - val_loss: 2.2263 - val_accuracy: 0.4456 - lr: 1.0000e-05\n",
      "Epoch 141/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2688 - accuracy: 0.4342 - val_loss: 2.2272 - val_accuracy: 0.4436 - lr: 1.0000e-05\n",
      "Epoch 142/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2692 - accuracy: 0.4311 - val_loss: 2.2275 - val_accuracy: 0.4450 - lr: 1.0000e-05\n",
      "Epoch 143/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2689 - accuracy: 0.4322 - val_loss: 2.2256 - val_accuracy: 0.4433 - lr: 1.0000e-05\n",
      "Epoch 144/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2705 - accuracy: 0.4316 - val_loss: 2.2253 - val_accuracy: 0.4428 - lr: 1.0000e-05\n",
      "Epoch 145/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2686 - accuracy: 0.4321 - val_loss: 2.2260 - val_accuracy: 0.4453 - lr: 1.0000e-05\n",
      "Epoch 146/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2663 - accuracy: 0.4337 - val_loss: 2.2257 - val_accuracy: 0.4430 - lr: 1.0000e-05\n",
      "Epoch 147/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2710 - accuracy: 0.4344 - val_loss: 2.2258 - val_accuracy: 0.4442 - lr: 1.0000e-05\n",
      "Epoch 148/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2689 - accuracy: 0.4330 - val_loss: 2.2261 - val_accuracy: 0.4436 - lr: 1.0000e-05\n",
      "Epoch 149/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2653 - accuracy: 0.4315 - val_loss: 2.2247 - val_accuracy: 0.4411 - lr: 1.0000e-05\n",
      "Epoch 150/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2645 - accuracy: 0.4316 - val_loss: 2.2246 - val_accuracy: 0.4425 - lr: 1.0000e-05\n",
      "Epoch 151/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2647 - accuracy: 0.4312 - val_loss: 2.2240 - val_accuracy: 0.4433 - lr: 1.0000e-05\n",
      "Epoch 152/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2697 - accuracy: 0.4261 - val_loss: 2.2240 - val_accuracy: 0.4436 - lr: 5.0000e-06\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2702 - accuracy: 0.4306 - val_loss: 2.2244 - val_accuracy: 0.4448 - lr: 5.0000e-06\n",
      "Epoch 154/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2672 - accuracy: 0.4315 - val_loss: 2.2249 - val_accuracy: 0.4453 - lr: 5.0000e-06\n",
      "Epoch 155/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2677 - accuracy: 0.4281 - val_loss: 2.2250 - val_accuracy: 0.4425 - lr: 5.0000e-06\n",
      "Epoch 156/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2700 - accuracy: 0.4289 - val_loss: 2.2242 - val_accuracy: 0.4433 - lr: 5.0000e-06\n",
      "Epoch 157/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2700 - accuracy: 0.4319 - val_loss: 2.2238 - val_accuracy: 0.4422 - lr: 5.0000e-06\n",
      "Epoch 158/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2693 - accuracy: 0.4293 - val_loss: 2.2240 - val_accuracy: 0.4430 - lr: 5.0000e-06\n",
      "Epoch 159/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2637 - accuracy: 0.4307 - val_loss: 2.2240 - val_accuracy: 0.4422 - lr: 5.0000e-06\n",
      "Epoch 160/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2649 - accuracy: 0.4341 - val_loss: 2.2243 - val_accuracy: 0.4433 - lr: 5.0000e-06\n",
      "Epoch 161/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2716 - accuracy: 0.4297 - val_loss: 2.2239 - val_accuracy: 0.4436 - lr: 5.0000e-06\n",
      "Epoch 162/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2661 - accuracy: 0.4306 - val_loss: 2.2246 - val_accuracy: 0.4442 - lr: 3.0000e-06\n",
      "Epoch 163/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2635 - accuracy: 0.4342 - val_loss: 2.2248 - val_accuracy: 0.4445 - lr: 3.0000e-06\n",
      "Epoch 164/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2712 - accuracy: 0.4277 - val_loss: 2.2243 - val_accuracy: 0.4433 - lr: 3.0000e-06\n",
      "Epoch 165/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2699 - accuracy: 0.4274 - val_loss: 2.2227 - val_accuracy: 0.4428 - lr: 3.0000e-06\n",
      "Epoch 166/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2690 - accuracy: 0.4301 - val_loss: 2.2239 - val_accuracy: 0.4442 - lr: 3.0000e-06\n",
      "Epoch 167/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2637 - accuracy: 0.4281 - val_loss: 2.2233 - val_accuracy: 0.4436 - lr: 3.0000e-06\n",
      "Epoch 168/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2658 - accuracy: 0.4317 - val_loss: 2.2237 - val_accuracy: 0.4433 - lr: 3.0000e-06\n",
      "Epoch 169/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2636 - accuracy: 0.4331 - val_loss: 2.2228 - val_accuracy: 0.4436 - lr: 3.0000e-06\n",
      "Epoch 170/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2678 - accuracy: 0.4309 - val_loss: 2.2232 - val_accuracy: 0.4439 - lr: 3.0000e-06\n",
      "Epoch 171/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2660 - accuracy: 0.4280 - val_loss: 2.2222 - val_accuracy: 0.4425 - lr: 3.0000e-06\n",
      "Epoch 172/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2673 - accuracy: 0.4278 - val_loss: 2.2232 - val_accuracy: 0.4450 - lr: 1.0000e-06\n",
      "Epoch 173/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2644 - accuracy: 0.4291 - val_loss: 2.2233 - val_accuracy: 0.4425 - lr: 1.0000e-06\n",
      "Epoch 174/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2673 - accuracy: 0.4312 - val_loss: 2.2227 - val_accuracy: 0.4430 - lr: 1.0000e-06\n",
      "Epoch 175/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2635 - accuracy: 0.4335 - val_loss: 2.2223 - val_accuracy: 0.4428 - lr: 1.0000e-06\n",
      "Epoch 176/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2613 - accuracy: 0.4320 - val_loss: 2.2231 - val_accuracy: 0.4436 - lr: 1.0000e-06\n",
      "Epoch 177/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2658 - accuracy: 0.4284 - val_loss: 2.2223 - val_accuracy: 0.4430 - lr: 1.0000e-06\n",
      "Epoch 178/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2597 - accuracy: 0.4341 - val_loss: 2.2241 - val_accuracy: 0.4459 - lr: 1.0000e-06\n",
      "Epoch 179/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2617 - accuracy: 0.4321 - val_loss: 2.2232 - val_accuracy: 0.4442 - lr: 1.0000e-06\n",
      "Epoch 180/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2627 - accuracy: 0.4306 - val_loss: 2.2225 - val_accuracy: 0.4425 - lr: 1.0000e-06\n",
      "Epoch 181/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2643 - accuracy: 0.4313 - val_loss: 2.2228 - val_accuracy: 0.4425 - lr: 1.0000e-06\n",
      "Epoch 182/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2627 - accuracy: 0.4340 - val_loss: 2.2225 - val_accuracy: 0.4425 - lr: 5.0000e-07\n",
      "Epoch 183/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2642 - accuracy: 0.4302 - val_loss: 2.2234 - val_accuracy: 0.4445 - lr: 5.0000e-07\n",
      "Epoch 184/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2663 - accuracy: 0.4349 - val_loss: 2.2231 - val_accuracy: 0.4430 - lr: 5.0000e-07\n",
      "Epoch 185/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2654 - accuracy: 0.4345 - val_loss: 2.2223 - val_accuracy: 0.4422 - lr: 5.0000e-07\n",
      "Epoch 186/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2719 - accuracy: 0.4270 - val_loss: 2.2232 - val_accuracy: 0.4436 - lr: 5.0000e-07\n",
      "Epoch 187/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2646 - accuracy: 0.4311 - val_loss: 2.2227 - val_accuracy: 0.4430 - lr: 5.0000e-07\n",
      "Epoch 188/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2677 - accuracy: 0.4287 - val_loss: 2.2222 - val_accuracy: 0.4433 - lr: 5.0000e-07\n",
      "Epoch 189/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2652 - accuracy: 0.4330 - val_loss: 2.2230 - val_accuracy: 0.4422 - lr: 5.0000e-07\n",
      "Epoch 190/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2660 - accuracy: 0.4274 - val_loss: 2.2226 - val_accuracy: 0.4425 - lr: 5.0000e-07\n",
      "Epoch 191/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2689 - accuracy: 0.4330 - val_loss: 2.2232 - val_accuracy: 0.4430 - lr: 5.0000e-07\n",
      "Epoch 192/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2657 - accuracy: 0.4338 - val_loss: 2.2229 - val_accuracy: 0.4442 - lr: 1.0000e-07\n",
      "Epoch 193/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2625 - accuracy: 0.4316 - val_loss: 2.2237 - val_accuracy: 0.4433 - lr: 1.0000e-07\n",
      "Epoch 194/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2636 - accuracy: 0.4340 - val_loss: 2.2238 - val_accuracy: 0.4442 - lr: 1.0000e-07\n",
      "Epoch 195/200\n",
      "657/657 [==============================] - 8s 12ms/step - loss: 2.2626 - accuracy: 0.4334 - val_loss: 2.2226 - val_accuracy: 0.4436 - lr: 1.0000e-07\n",
      "Epoch 196/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2617 - accuracy: 0.4297 - val_loss: 2.2235 - val_accuracy: 0.4470 - lr: 1.0000e-07\n",
      "Epoch 197/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2608 - accuracy: 0.4333 - val_loss: 2.2224 - val_accuracy: 0.4442 - lr: 1.0000e-07\n",
      "Epoch 198/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2624 - accuracy: 0.4321 - val_loss: 2.2229 - val_accuracy: 0.4430 - lr: 1.0000e-07\n",
      "Epoch 199/200\n",
      "657/657 [==============================] - 7s 11ms/step - loss: 2.2647 - accuracy: 0.4310 - val_loss: 2.2228 - val_accuracy: 0.4436 - lr: 1.0000e-07\n",
      "Epoch 200/200\n",
      "657/657 [==============================] - 8s 11ms/step - loss: 2.2670 - accuracy: 0.4269 - val_loss: 2.2228 - val_accuracy: 0.4430 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=200,\n",
    "            batch_size=32,\n",
    "            callbacks=[lr_scheduler]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d7de7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 1s 6ms/step - loss: 2.2228 - accuracy: 0.4430\n",
      "Test loss: 2.2228288650512695\n",
      "Test accuracy: 0.4430488049983978\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "108e78d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/110 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff5b220f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04138707, 0.06039013, 0.11357411, ..., 0.10891047, 0.15546967,\n",
       "        0.1712307 ],\n",
       "       [0.14307354, 0.07237261, 0.02740956, ..., 0.5379369 , 0.09588666,\n",
       "        0.04353119],\n",
       "       [0.05607066, 0.00543116, 0.03357522, ..., 0.01596706, 0.04808548,\n",
       "        0.45700195],\n",
       "       ...,\n",
       "       [0.15938637, 0.16355582, 0.03104691, ..., 0.22989476, 0.14316382,\n",
       "        0.17440644],\n",
       "       [0.1702728 , 0.09815435, 0.15182802, ..., 0.2694157 , 0.1877216 ,\n",
       "        0.02708928],\n",
       "       [0.02271451, 0.05910092, 0.10607969, ..., 0.02412572, 0.05790017,\n",
       "        0.01738121]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ff75350",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c214e939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 7, ..., 5, 5, 4], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d556df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0626601",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a74d1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[142  51  40  31   3  51  36  19]\n",
      " [ 46 145  20  13  40  72  22  27]\n",
      " [ 27   8  47  10  13  12  14   9]\n",
      " [ 13   3  17  60   1  11  15  35]\n",
      " [ 20  44  50  38 511  27  23  35]\n",
      " [ 79  85  80  76  58 339 157  96]\n",
      " [ 19  12  15  20   8  25  65   4]\n",
      " [ 30  17  31 135  18  58  32 243]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "860bde04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 219   31   75  100   35]\n",
      " [ 123 1290  147  117   71]\n",
      " [ 177  112  637  284   78]\n",
      " [ 121   53  187  337   26]\n",
      " [  53   49   46   26  434]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67dbbec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38       373\n",
      "           1       0.40      0.38      0.39       385\n",
      "           2       0.16      0.34      0.21       140\n",
      "           3       0.16      0.39      0.22       155\n",
      "           4       0.78      0.68      0.73       748\n",
      "           5       0.57      0.35      0.43       970\n",
      "           6       0.18      0.39      0.24       168\n",
      "           7       0.52      0.43      0.47       564\n",
      "\n",
      "    accuracy                           0.44      3503\n",
      "   macro avg       0.39      0.42      0.39      3503\n",
      "weighted avg       0.51      0.44      0.46      3503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fd97f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.48      0.38       461\n",
      "           1       0.83      0.74      0.78      1714\n",
      "           2       0.63      0.48      0.54      1428\n",
      "           3       0.35      0.50      0.41       610\n",
      "           4       0.66      0.70      0.68       615\n",
      "\n",
      "    accuracy                           0.60      4828\n",
      "   macro avg       0.56      0.58      0.56      4828\n",
      "weighted avg       0.64      0.60      0.61      4828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c27f1c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.48      0.38       460\n",
      "           1       0.84      0.74      0.79      1748\n",
      "           2       0.58      0.49      0.54      1288\n",
      "           3       0.39      0.47      0.42       724\n",
      "           4       0.67      0.71      0.69       608\n",
      "\n",
      "    accuracy                           0.60      4828\n",
      "   macro avg       0.56      0.58      0.56      4828\n",
      "weighted avg       0.63      0.60      0.61      4828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "142566a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.46      0.47       463\n",
      "           1       0.57      0.60      0.58       387\n",
      "           2       0.39      0.48      0.43       267\n",
      "           3       0.38      0.48      0.42       368\n",
      "           4       0.90      0.87      0.89       750\n",
      "           5       0.88      0.75      0.81       880\n",
      "           6       0.45      0.46      0.45       426\n",
      "           7       0.53      0.51      0.52       601\n",
      "\n",
      "    accuracy                           0.62      4142\n",
      "   macro avg       0.57      0.58      0.57      4142\n",
      "weighted avg       0.64      0.62      0.63      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be5c0d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.47      0.48       475\n",
      "           1       0.55      0.61      0.58       370\n",
      "           2       0.37      0.48      0.42       256\n",
      "           3       0.40      0.47      0.43       393\n",
      "           4       0.90      0.87      0.88       750\n",
      "           5       0.89      0.75      0.81       894\n",
      "           6       0.45      0.46      0.46       424\n",
      "           7       0.54      0.54      0.54       580\n",
      "\n",
      "    accuracy                           0.62      4142\n",
      "   macro avg       0.58      0.58      0.58      4142\n",
      "weighted avg       0.64      0.62      0.63      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c5aa387",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mlp2_dataset_60%_5emotions.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877ea5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvcourse",
   "language": "python",
   "name": "cvcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
